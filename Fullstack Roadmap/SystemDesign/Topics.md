1. Vertical Scaling

Analogy:
You have one chef in a small pizza shop. As orders increase, you ask the chef to work harder â€” get better tools or pay them overtime â€” so they produce more pizzas.

System Design Meaning:
Increasing the capacity of a single server (e.g., adding more CPU, memory, or faster disks). More powerful hardware handles more load. This is vertical scaling.

Web Development Perspective:
For a web app, this means upgrading your server instance (e.g., larger AWS EC2 machine, more memory). It works up to a point but has limits â€” a single machine can only scale so far.

2. Preprocessing / Scheduled Work

Analogy:
When the shop is empty (like early mornings), the chef prepares dough, so later orders are faster.

System Design Meaning:
Do heavy or repetitive work during low-traffic times (e.g., background jobs, batch processing, or cron jobs) so that peak times arenâ€™t slowed down.

Web Dev Perspective:
Use background workers (Node workers, AWS Lambda, Sidekiq, etc.) to preprocess data, precompile assets, or warm caches early so web servers are free when users are active.

3. Redundancy & Backup

Analogy:
Your only chef calls in sick â€” the shop collapses. So you hire a backup chef who takes over if the main chef fails.

System Design Meaning:
Add redundant servers or components so that if one fails, another continues service. This avoids single points of failure.

Web Dev Perspective:
Multiple web servers behind a load balancer, database replicas, and failover servers ensure uptime even if one instance goes down.

4. Horizontal Scaling

Analogy:
Instead of overworking one chef, hire more chefs and split orders among them.

System Design Meaning:
Add more identical machines (servers) that serve requests in parallel â€” this is horizontal scaling.

Web Dev Perspective:
Deploy multiple copies of your web app (e.g., in Kubernetes pods, cloud auto-scaling groups) so many users can be served simultaneously.

5. Microservices

Analogy:
Instead of one chef doing everything, specialized chefs handle pizza, garlic bread, or desserts.

System Design Meaning:
Each piece of functionality (e.g., authentication, orders, payments) lives in its own service. This lets each one scale independently and deploy separately.

Web Dev Perspective:
Instead of a single monolithic backend, your web app can use separate services for user management, orders, data storage, etc., often communicating via APIs.

6. Distributed Systems

Analogy:
Your pizza business expands to multiple locations so a single outage doesnâ€™t shut the whole business down.

System Design Meaning:
Running systems across multiple machines or data centers so no failure can take down the entire application.

Web Dev Perspective:
Use geographically distributed servers and databases so users around the world get faster responses and outages are isolated.

7. Load Balancing

Analogy:
A dispatcher routes pizza orders to whichever shop (or chef) is least busy.

System Design Meaning:
A load balancer routes incoming traffic intelligently across multiple servers.

Web Dev Perspective:
AWS ELB, Google Cloud Load Balancers, or NGINX can distribute HTTP requests so no one server gets overwhelmed.

8. Decoupling

Analogy:
The delivery driver doesnâ€™t need to know how the pizza is cooked â€” just pick it up and deliver.

System Design Meaning:
Break a system into parts with well-defined interfaces; components donâ€™t depend directly on each otherâ€™s internal logic.

Web Dev Perspective:
Using APIs and messaging queues ensures that changes in one part of your app donâ€™t break others.

9. Logging & Metrics

Analogy:
If deliveries are slow, you check which part of the process is slowing down (oven, driver, etc.).

System Design Meaning:
Collect detailed logs and performance metrics to trace issues, understand load, and debug problems.

Web Dev Perspective:
Tools like Prometheus, Grafana, ELK Stack, or cloud monitors help you understand app performance and failures.

10. Extensibility

Analogy:
Your pizza shop adds burgers tomorrow â€” if your systems are well-designed, you donâ€™t rewrite everything to add a new menu item.

System Design Meaning:
Architect systems so theyâ€™re easy to extend with new features or modules without massive rewrites.

Web Dev Perspective:
Design your web app with plugins, feature flags, modular components â€” so adding new business logic is straightforward.

# Vertical Vs Horizontal Scaling
1. What System Design Is

Explanation from Video:
System design starts when you take an algorithm or piece of code and make it available to other users â€” not just on your computer, but over the network using an API. Your algorithm becomes a service that runs on servers accessible online.

Analogy Used:
Though not explicitly described with a story like other system-design videos, the idea is that your code is like a service provider â€” you donâ€™t hand people your laptop, you expose functions via APIs so others can make requests from anywhere.

Web Dev Connection:
This is exactly what web APIs do â€” they let clients (browsers, mobile apps) send HTTP requests to your service to fetch data or do work.

2. Cloud vs Local Computer

Concept:
The video contrasts running your code on your personal computer versus deploying it in the cloud. The cloud gives resilience, configurability, and reliability you donâ€™t get on a simple desktop.

Web Dev Connection:
Most real-world web applications are deployed to cloud providers (AWS, GCP, Azure), not local machines, because cloud platforms offer load balancing, auto-scaling, and global access.  
3. Vertical Scaling (Scale Up)

What It Means:
Vertical scaling means increasing the capacity of a single server â€” e.g., adding more CPU, RAM, or disk to it so it can serve more users.

Analogy (Implicit from Screencaps & Notes):
Think of it as making one chef stronger. If the chef is overwhelmed, you give them better tools or more energy so they can cook faster. You donâ€™t add more chefs â€” you just make the one supplier better.

Web Dev Connection:
Upgrading the server your app runs on (e.g., moving from a small VPS to a beefier instance) is vertical scaling. Itâ€™s simple but limited â€” a single machine can only grow so big.
4. Horizontal Scaling (Scale Out)

What It Means:
Horizontal scaling means adding more servers so that the incoming requests can be handled in parallel.

Analogy (Taken from Context Around Concept):
Instead of one overwhelmed chef, you hire multiple chefs, each cooking simultaneously. An ordering system (like a dispatcher) decides which chef takes which order so all chefs are productive.

Web Dev Connection:
This is how most large systems work: multiple instances of a web app behind a load balancer serve requests concurrently. It increases overall capacity and fault tolerance.
5. Trade-offs Between Vertical and Horizontal Scaling

The video emphasizes the pros and cons of both approaches:

Vertical Scaling Pros:

Simpler setup

Fast internal communication (everythingâ€™s on one machine)

Data stays consistent easily

Vertical Scaling Cons:

Single point of failure

Limited by hardware capacity

Horizontal Scaling Pros:

Many machines means no single point of failure

Better for handling large workloads

Horizontal Scaling Cons:

Requires load balancing

Inter-machine communication adds overhead

Web Dev Connection:
When you design a web backend, you choose vertical scaling early on for simplicity, but eventually adopt horizontal scaling with load balancers and autoscaling groups to handle traffic spikes.
6. Load Balancing

Mentioned Concept:
To use multiple machines effectively, you need a load balancer â€” a system that routes requests to different servers.

Analogy (from surrounding context):
Itâ€™s like someone at the front of a restaurant directing customers to different chefs based on whoâ€™s free â€” this spreads load and prevents overload on one chef.

Web Dev Connection:
Cloud providers offer load balancers (AWS ELB/NLB, GCP Load Balancing) that distribute HTTP requests evenly across your fleet of servers.
7. High-Level vs Low-Level Design

Towards the end, the video contrasts High-Level Design (HLD) and Low-Level Design (LLD):

HLD: Big picture â€” components, services, data flow

LLD: Detailed implementation â€” classes, modules, APIs

Web Dev Connection:
When designing a web app, HLD might involve choosing microservices and CDNs, while LLD is designing your REST endpoints and database schema.

# Hashing In Load Balancer  
Setup Scenario
You have:
3 cache servers
C1
C2
C3
You store user profiles in cache.
To decide where to store a profile, you use regular hashing (modulo hashing):
server = hash(userId) % numberOfServers
So if there are 3 servers:
server = hash(userId) % 3
ğŸ”¢ Step 1: How Regular Hashing Works
Assume:
hash(101) = 1001
hash(102) = 1002
hash(103) = 1003
hash(104) = 1004
Now assign them:
1001 % 3 = 2 â†’ C2
1002 % 3 = 0 â†’ C0
1003 % 3 = 1 â†’ C1
1004 % 3 = 2 â†’ C2
So:
User	Server
101	C2
102	C0
103	C1
104	C2
Everything is working fine.
Profiles are cached.
â• Step 2: Add a New Server
Now traffic increases.
You add C3.
Now total servers = 4
The formula becomes:
server = hash(userId) % 4
Now recompute:
1001 % 4 = 1
1002 % 4 = 2
1003 % 4 = 3
1004 % 4 = 0
New mapping:
User	Old Server	New Server
101	C2	C1
102	C0	C2
103	C1	C3
104	C2	C0
ğŸš¨ Almost everyone moved.
ğŸ’¥ Why Almost All Keys Change
This is the core problem.
ğŸš¨ What Happens to Cache?
Now imagine real system.
Before adding server:
User 101 profile was stored in C2.
After adding server:
System now believes user 101 belongs to C1.
So when frontend requests:
GET /profile/101
System looks in:
C1
But profile is actually in:
C2
Result?
âŒ Cache miss.
System goes to database.
Loads profile again.
Stores it in C2.
Cache in c1 will expire over time.
ğŸ“‰ Why This Causes Massive Cache Invalidation
Letâ€™s say:
You had 10 million cached profiles.
You add one server.
~75â€“100% of mappings change.
So:
Cache lookup misses spike.
Database traffic explodes.
Latency increases.
System may crash under load.
Even though you added a server to improve capacityâ€¦
You just destroyed your cache efficiency.
Thatâ€™s the irony.
ğŸ¯ Concrete Profile Fetch Flow
Before new server:
User â†’ Load Balancer â†’ App â†’ hash(userId)%3 â†’ C2 â†’ Profile found â†’ return
After new server:
User â†’ Load Balancer â†’ App â†’ hash(userId)%4 â†’ C1 â†’ MISS
Then:
App â†’ Database â†’ Fetch profile â†’ Store in C1 â†’ Return
Now database suddenly receives millions of requests.
This is called:
ğŸ”¥ Cache churn
ğŸ”¥ Cold cache problem
ğŸ”¥ Rehash storm
ğŸ§® Probability View
If you had:
n servers
Add 1 server
Probability that a key stays in same bucket:
Very small.
In practice:
~ (1 / new_server_count)
Example:
3 â†’ 4 servers
Only about 1/4 keys might accidentally remain same.
~75% keys remap.
With 100 servers â†’ 101 servers
Almost 99% remap.
It gets worse as system grows.
ğŸ§  Why This Is Terrible for Profile Fetching
Profiles are:
Frequently accessed
Repeatedly accessed
Often read-heavy
You depend on high cache hit rate.
If cache hit rate drops from:
95% â†’ 10%
Your DB load increases by:
9x
And thatâ€™s how systems melt.
ğŸ Final Intuition
Regular hashing:
hash(key) % n
When n changes â†’ entire function changes.
So:
Keys shift unpredictably.
Cache becomes useless.
Database gets hammered.
Thatâ€™s why consistent hashing was invented:

2. The Core Idea of Consistent Hashing
Instead of:
hash(key) % n_servers
Consistent hashing treats both keys and servers on a circular hash ring.
Each server is hashed to a position on the ring
Each key is hashed to a position on the same ring
Keys map to the next server clockwise on the ring
This means:
When a server is added â†’ only its neighboursâ€™ range keys shift
When a server is removed â†’ only the keys that mapped to it are affected
This dramatically reduces the number of keys that move â€” typically only a small fraction, not all of them.
3. The Intuition / Analogy
While some videos have informal analogies (like people seated around a table), consistent hashing is best visualized with the ring model:
Imagine a circular scale from 0 to max hash value
Servers are dots randomly placed on this circle
â€œHash(key)â€ gives you a point on the circle
You walk clockwise until you hit the next server
Thatâ€™s where the key belongs.
This is like:
ğŸš¶â€â™‚ï¸ You start walking from a position (the key) clockwise â†’ the next chef (server) handles your order.
Only when you insert a new chef in between do the nearby orders change chefs â€” not all of them.
4. How Consistent Hashing Minimizes Remapping
In regular hashing:
hash(key) % n_servers
If n changes, nearly all keys change the server mapping.
In consistent hashing:
Keys far away from the added/removed server keep their original server
Only keys that belong to that serverâ€™s interval on the ring shift
Thus only ~1/N keys are remapped when one of N servers changes
This is a major improvement in scalability.
When a server is added/removed (or fails/rejoins), only a subset of requests change their mapping â€” not almost every one.
This keeps:
Cache hit rate high
Databases from being overloaded
Requests directed predictably
Consistent hashing isnâ€™t just a theory â€” itâ€™s used in production systems:
âœ” Distributed cache systems (e.g., Memcached clusters)
âœ” Distributed key-value stores (like Cassandra and Dynamo)
âœ” Load balancing logic requiring sticky sessions
âœ” Sharded databases where you donâ€™t want to reshuffle data on every scale change
| Concept         | Regular Hashing        | Consistent Hashing                             |
| --------------- | ---------------------- | ---------------------------------------------- |
| Key Mapping     | `hash(key) % n`        | Hash on a ring then *nearest clockwise server* |
| Server Scaling  | Causes huge remapping  | Only nearby keys remapped                      |
| Cache Stability | Very low after scaling | High after scaling                             |
| Typical Use     | Stateless routing      | Distributed caches & sharding                  |

Letâ€™s walk it step-by-step using:
Request: R1
Servers: S1, S2
Using consistent hashing
S1 initially owns R1
Then S1 is removed
System does:
Hash R1
Find next server clockwise
That is now S2
Check S2
MISS (because R1 was stored in S1, which is gone)
So:
Fetch from DB
Store in S2
Return response

TTL-Based (Most Common)
Each cached entry has:
expires_in = 10 minutes
If S1 was removed permanently:
Its cached data is lost
R1 will repopulate in S2
No special cleanup required
System self-heals.
EVen if request is routed to new server. The cached data will be invalidated in old server Via TTL
So S1â€™s cache for R1 becomes:
âŒ orphaned
âŒ dead cache
âŒ wasted memory (until TTL)
TTL = Time To Live
In caching, TTL means:
â³ How long a cached value is allowed to stay valid before it expires automatically.

# Message Queue
A Message Queue (MQ) is a system that allows different parts of an application to communicate asynchronously.
Instead of:
â€œDo this work right now and wait.â€
It says:
â€œHereâ€™s the work. Do it whenever youâ€™re ready.â€
It decouples who sends the work from who processes it.
ğŸ” Core Concepts (All the Pieces Together)
1ï¸âƒ£ Producer
The component that creates messages.
Example:
User places order
Web server produces:
{ orderId: 123, action: "processPayment" }
Producer does NOT process the task.
It just sends it to the queue.
# Queue
The buffer that stores messages temporarily.
Acts like a waiting line.
Usually FIFO (First In First Out).
Can persist messages to disk.
Absorbs traffic spikes.
It protects your system from overload.
3ï¸âƒ£ Consumer
The worker that reads messages and processes them.
Example:
Payment service
Email service
Image processing worker
Consumers can scale horizontally.
Add more consumers â†’ more throughput.
4ï¸âƒ£ Asynchronous Processing
This is the main purpose.
Instead of:
User â†’ Process everything â†’ Respond
We do:
User â†’ Put message in queue â†’ Respond immediately
Actual processing happens later.
Benefits:
Faster response time
Better scalability
Reduced blocking
5ï¸âƒ£ Notifier (Avoid Polling)
Problem:
Without notification, consumers keep asking:
â€œIs there work?â€
â€œIs there work?â€
This is called polling â€” wasteful.
Solution:
A Notifier alerts consumers when a message arrives.
Like a bell ringing in a kitchen when a new order comes.
Result:
Efficient wake-up
No unnecessary CPU usage
6ï¸âƒ£ Load Balancer (Distribute Work)
When you have multiple consumers:
Without coordination:
One worker overloaded
Others idle
Load balancer ensures:
Messages distributed evenly
Fair work sharing
Horizontal scaling
Add workers â†’ system capacity increases.
7ï¸âƒ£ Heartbeat Mechanism (Failure Detection)
Problem:
Consumer picks a messageâ€¦
Then crashes.
Now message is stuck.
Solution:
Consumers periodically send:
â€œIâ€™m aliveâ€
If heartbeat stops:
System detects failure
Message is reassigned
Work is not lost
This ensures reliability.
8ï¸âƒ£ Delivery Guarantees
Different systems provide different guarantees:
At-most-once â†’ May lose messages
At-least-once â†’ May duplicate messages
Exactly-once â†’ Hardest to implement
Real systems usually use at-least-once.
9ï¸âƒ£ Why Message Queues Matter
They solve:
Problem	How MQ Helps
Traffic spikes	Queue buffers load
Slow services	Async decoupling
System crashes	Retry via heartbeat
Tight coupling	Producer & consumer independent
Scaling	Add more consumers
ğŸ— Real Web Example (Putting It All Together)
User uploads image.
Instead of resizing immediately:
App â†’ pushes message to queue
Queue stores message
Notifier alerts worker
Load balancer assigns worker
Worker sends heartbeat
Worker processes image
If worker crashes â†’ job reassigned
User experience:
Fast upload confirmation.
System experience:
Controlled background processing.
ğŸ”¥ Big System Design Insight
Message Queue =
Buffer + Decoupler + Failure Recovery + Load Distributor
It is NOT just a list of tasks.
It is an orchestration mechanism for distributed systems.
ğŸ§© When You Should Use It
Use MQ when:
Work doesnâ€™t need immediate completion
System experiences bursts
Tasks are long-running
Services need decoupling
Reliability matters
Avoid MQ when:
You need real-time synchronous consistency
Latency must be near-zero
Comparison: Kafka vs RabbitMQ vs SQS

# Monolith Vs Microservice
1. What Is a Monolithic Architecture?
A monolithic application is a single unified application where:
All functionality lives in one deployable unit
All components run as one process
Business logic, UI, and database access are packaged together
This style is simple and familiar â€” you build one application and deploy it.
In web development, this could be:
User service + Product service + Order service
     all inside a single codebase and deployed together
2. What Is a Microservices Architecture?
In contrast, microservices break the system into many small, independent services.
Each service:
Owns a single business capability
Runs in its own process
Communicates over lightweight protocols (like HTTP/gRPC)
Can be scaled, deployed, and updated independently
So the same example becomes:
User service (own DB) 
Product service (own DB) 
Order service (own DB)
Each runs and scales on its own.
3. Why Use Microservices?
âœ… a. Independent Deployment
Microservices let you deploy features separately.
Fix user service without touching order service
Roll back product service independently
This boosts development velocity.
âœ… b. Scalability
You can scale only the parts that need it:
If order load spikes â†’ scale order service
No need to scale everything as in a monolith
This is powerful in distributed systems.
âœ… c. Better Fault Isolation
If one service fails:
It doesnâ€™t necessarily bring down the whole system
Other services continue functioning
This improves reliability.
âœ… d. Technology Diversity
Each service can use its own stack:
Java for order service
Node.js for user service
Python for reporting
This makes teams more flexible.
âš ï¸ 4. Downsides of Microservices
However, microservices also bring challenges:
âŒ a. Distributed Complexity
Microservices are a distributed system:
Network calls instead of simple function calls
More points of failure
Need load balancing, service discovery, circuit breakers
This increases operational complexity.
âŒ b. Data Consistency
Each service may have its own database
Maintaining consistency across services is harder
You need patterns like eventual consistency or sagas
âŒ c. Deployment Complexity
More services â†’ more deployment pipelines
More coordination required
Higher maintenance overhead
âŒ d. Monitoring + Debugging
Debugging across services is harder than tracing within one process.
5. When Monolith Is Preferred Over Microservices
The video emphasizes that microservices are not always the right choice.
You should prefer a monolith when:
ğŸŸ¢ 1. The application is small or simple
If the system is modest in scope, splitting into microservices adds unnecessary complexity.
ğŸŸ¢ 2. The team is small
With one or two developers, managing many services is overkill:
Hard to maintain multiple repos
Harder to test distributed changes
In that case, a monolith is easier to reason about.
ğŸŸ¢ 3. Thereâ€™s no need to scale parts independently
If all parts of the system have similar load patterns:
Independent scaling may not add much benefit
A monolith with straightforward scaling might be enough
ğŸŸ¢ 4. Want simpler deployment and testing
A monolith can:
Be tested end-to-end more easily
Be deployed in one go
Avoid complex service coordination
Many startups begin with a monolith for this reason.
This is a core trade-off highlighted in the video.
ğŸ” 6. Justification for Microservices (When They Make Sense)
The video points out that microservices are most justified when:
ğŸŸ¡ a. The system is large and complex
As complexity grows:
One codebase becomes hard to maintain
Changes in one module may break others
Teams struggle without isolation
So microservices bring modularity and separation of concerns.
ğŸŸ¡ b. Independent Scaling Requirements
Different parts may have different usage patterns:
Example (eCommerce):
Search service (high read)
Order service (high write)
Inventory service (medium)
Each has different performance characteristics â€” microservices let you scale them independently.
ğŸŸ¡ c. Multiple Teams Working Concurrently
Large organizations:
Separate teams own separate services
Fewer merge conflicts
Teams can release on independent cycles
This improves throughput.
ğŸŸ¡ d. Domain-driven Design (DDD) Fit
When architecture aligns with business domains:
User domain
Product domain
Order domain
Microservices map nicely to bounded contexts.
ğŸ§  7. How Microservices Addresses Issues of Monoliths
Problem with Monolith	Microservices Benefit
Hard to Scale selectively	Independent scalability
Hard to isolate failures	Fault isolation
Large deployment units	Smaller, frequent deploys
Slower team velocity	Independent development
One language stack	Polyglot services
Microservices are a pattern for scaling development, not just traffic.
ğŸš¦ Summary of When to Use Which
Monolith (Prefer When)
âœ” Application scope is small
âœ” Team is small
âœ” You want simpler deployment and testing
âœ” You donâ€™t need independent scaling
âœ” You want easier distributed transactions
âœ” WHen all your server interactions are within same db. S1 always talks to S2
Monolith wins for simplicity and early stage.
Microservices (Prefer When)
âœ” System is large and complex
âœ” Multiple teams
âœ” Need independent scaling
âœ” Want fault isolation
âœ” Domain complexity demands modularization
Microservices win for scale and team autonomy.

# Database Sharding
What Is Database Sharding?
Database sharding is the process of splitting a large database into smaller, more manageable pieces called shards. Each shard holds only a portion of the total dataset, and usually sits on a separate database server instance.
Itâ€™s a form of horizontal partitioning â€” data is split by rows, not columns â€” so each shard handles a subset of the entire table(s).
Why Sharding Is Needed
When your app scales:
One database becomes too large
Table scan and index operations slow down
Queries take longer
The database becomes a bottleneck
Even with indexing and optimization, a single database eventually fails to handle huge traffic and massive data growth.
Sharding solves this by distributing load and data across multiple servers, improving scalability and performance.
Tinder Example (Common Real-World Case)
Letâ€™s imagine how Tinder might store user profiles and match data before sharding.
ğŸ§± Original (Pre-Sharding) Database
Users Table
-----------------
user_id
name
age
location
preferences
photo_urls
last_active
...
Matches Table
-----------------
match_id
user_id_1
user_id_2
timestamp
All millions of users and matches are stored in one large database.
ğŸ“‰ Query Example Before Sharding
Suppose:
SELECT * FROM Users
WHERE location = "New York";
If Users table has 100M rows:
The DB searches through millions of rows
Even with indexing, itâ€™s large data
Query time could be tens to hundreds of milliseconds, or slower
For large joins (e.g., users + matches), time grows significantly
This leads to:
Slow response times
Heavy load on DB
Poor user experience
ğŸ†• After Sharding
Instead of one massive DB, you split into shards.
A shard might be based on a logical shard key such as:
âœ” location
âœ” user_id % N
So Tinder might shard by location:
Shard1 â†’ Users from US & Canada
Shard2 â†’ Users from Europe
Shard3 â†’ Users from Asia
...
Or by user_id % 4, where each user_id maps to one of 4 shards.
Each shard holds a subset of the data.
How Query Changes After Sharding
Now if you run:
SELECT * FROM Users
WHERE location = "New York";
Instead of scanning the entire database, the system:
Determines which shard uses that location
Routes the query only to that shard
Only that shard scans its smaller dataset
This drastically reduces the query time because:
A shard might have only 10 million users instead of 100 million
Search and indexing are faster
Index smaller size â†’ faster lookup
âš™ï¸ How Sharding Works Mechanically
You need:
â¤ Shard Key
A field that determines which shard a row belongs to. Examples:
user_id % N
geographic region
hashed value
The key must distribute data evenly to avoid hot spots.
â¤ Shard Manager / Metadata
A component that maps shard keys to shard servers.
Example:
Location â†’ Shard ID
Shard ID â†’ DB server endpoint
When an app queries, it checks the shard manager first to know where to send the request.
â¤ Router Logic
At runtime, application logic:
shard_id = shard_key % N
send query to Shard[shard_id]
This ensures queries go only to the relevant shard, not all of them.
âš ï¸ Trade-offs and Challenges
While sharding improves scalability, it adds complexity:
âŒ Cross-Shard Joins
If your query needs data from multiple shards, you may have to query all shards and aggregate results. This is slow and complex.
âŒ Resharding Difficulty
Adding new shards requires redistributing data. Unless you plan ahead with techniques like consistent hashing, resharding can be expensive.
âŒ Hot Spots
If your shard key doesnâ€™t distribute evenly (e.g., most users in New York), some shards get overloaded.
âŒ Operational Overhead
You now have multiple databases to maintain, backup, monitor, and replicate.
Final Thought
Sharding is a horizontal scaling technique that trades added system complexity for major improvements in performance and scalability. For apps with huge datasets (like social apps, dating apps, globally distributed user bases), it turns an unscalable monolith into a scalable, distributed system.
Increases INfra cost
1 RDS + 1 standby
Shard1 â†’ Primary + Standby
Shard2 â†’ Primary + Standby
Shard3 â†’ Primary + Standby
Shard4 â†’ Primary + Standby
So yes â€” cost multiplies.
Sharding increases:
Infra cost
Operational complexity
Monitoring overhead
Shard Manager can Live in my backedn service itself
Your application itself contains shard logic:
shardId = userId % 4
connectTo(shardId)
No separate server required.
Shard logic lives inside:
API service
Backend service
This is the most common approach.
No extra infrastructure needed

# Caching
Caching means storing frequently accessed or expensive-to-compute data in fast-access memory (closer to the application) instead of always hitting a slower backend (usually a database).
Instagram news feed example (main analogy throughout):
User requests their feed â†’ server queries DB for followed users + their posts.
Typical latency breakdown (rough numbers used):
Client â†” server: ~100 ms each way
Server â†” DB: ~10 ms each way
Total round-trip: ~220 ms
Goal: Reduce the server-to-DB part dramatically (from 20 ms to ~1â€“2 ms).
By caching the computed feed (or parts of it):
First request: Compute from DB â†’ store result in cache â†’ return to user.
Next similar requests: Serve directly from cache (1 ms instead of 20 ms).
Result: Much lower latency, huge savings when hit rate is high.
Key Benefits Demonstrated
Reduces latency dramatically for repeated/similar queries.
Saves computation â€” avoid re-doing expensive work (joins, aggregations, ranking).
Improves scalability â€” database gets far fewer requests.
Can apply at multiple levels:
Backend services (in-memory like Redis, Memcached)
Even client-side (mobile app caches feed locally â†’ instant reload when reopening app)
Limitations & Trade-offs
You can't cache everything â€” large databases (TB/PB scale) won't fit in memory.
â†’ You cache only the "hot" / frequently accessed subset.
â†’ Effectiveness depends on access pattern (power-law / Zipf distribution: small % of data is requested most of the time).
If cache miss rate is high â†’ you pay extra overhead (check cache + then DB).
Two big policy questions every cache must solve:
Write / Update policy (Cache-Aside, Write-Through, Write-Behind, etc. â€” how/when to update or invalidate cache when source data changes).
Eviction policy (when cache is full, what to remove?):
LRU (Least Recently Used) â€” most common
LFU (Least Frequently Used)
Others (ARC, ML-based, etc.)
Takeaways
Caching trades storage (extra memory) for speed and reduced load.
It's simple in concept but powerful â€” tiny changes (1 ms vs 10 ms) compound massively at scale.
Real-world systems use multi-level caching (CDN edge, app server local cache, distributed cache like Redis, database query cache).
The video teases deeper follow-ups on specific write policies, eviction strategies, consistency issues (stale data), cache stampedes/thundering herd, etc.